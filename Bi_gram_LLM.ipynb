{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Xin-T3BXUlRE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "max_iters = 10000\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the file into memory\n",
        "with open('/content/sample_data/wizard_of_oz.txt', 'r', encoding = 'utf-8') as f:\n",
        "\ttext = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "vocabulary_size = len(chars)\n"
      ],
      "metadata": {
        "id": "kLHtfS67Vbe-"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#manual encoding of the data:\n",
        "\n",
        "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
        "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text))\n",
        "\n",
        "print(data[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3HQ5wwWV8Zx",
        "outputId": "af24b5d0-0f52-4ffc-fc30-702d8eebd2c0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([78,  0,  0, 38, 55, 66, 58, 51, 16,  1, 38, 54, 51,  1, 41, 61, 60, 50,\n",
            "        51, 64, 52, 67, 58,  1, 41, 55, 72, 47, 64, 50,  1, 61, 52,  1, 33, 72,\n",
            "         0,  0, 19, 67, 66, 54, 61, 64, 16,  1, 30,  9,  1, 24, 64, 47, 60, 57,\n",
            "         1, 20, 47, 67, 59,  0,  0, 36, 51, 58, 51, 47, 65, 51,  1, 50, 47, 66,\n",
            "        51, 16,  1, 24, 51, 48, 64, 67, 47, 64, 71,  1, 11,  7,  1, 11, 15, 15,\n",
            "        13,  1, 45, 51, 20, 61, 61, 57,  1,  3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the split into train and test set -80/20 split:\n",
        "\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "Ok8SbR1WWIsl"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#getting our x and y from both the train and test set:\n",
        "\n",
        "block_size = 8\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]"
      ],
      "metadata": {
        "id": "53HVdO4zWJyN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now a manual illustration of how the bigram model works:\n",
        "for t in range(block_size):\n",
        "\tcontext = x[:t+1]\n",
        "\ttarget = y[t]\n",
        "\tprint('when input is', context, 'target is ', target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eT7QLlyWMYG",
        "outputId": "4440444f-4c38-455f-a807-9cc81f558af0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([78]) target is  tensor(0)\n",
            "when input is tensor([78,  0]) target is  tensor(0)\n",
            "when input is tensor([78,  0,  0]) target is  tensor(38)\n",
            "when input is tensor([78,  0,  0, 38]) target is  tensor(55)\n",
            "when input is tensor([78,  0,  0, 38, 55]) target is  tensor(66)\n",
            "when input is tensor([78,  0,  0, 38, 55, 66]) target is  tensor(58)\n",
            "when input is tensor([78,  0,  0, 38, 55, 66, 58]) target is  tensor(51)\n",
            "when input is tensor([78,  0,  0, 38, 55, 66, 58, 51]) target is  tensor(16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, as you can see above, as we seen in successive iterations, the target is always the next character in the text. So this is the kind of algorithm we want to set up for the whole text corpus of Wizard of Oz."
      ],
      "metadata": {
        "id": "6gr6UoE2WTq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Bigram model using Pytorch"
      ],
      "metadata": {
        "id": "Rybxmf7qYjQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now setting up a batch_size for the traing, to make it faster, we can't be training the model one by one ofcourse, as now we want to implement the model using vectorization thru the use of the batch_size:"
      ],
      "metadata": {
        "id": "Mvh8b2NQYTaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#now the batch_size & also getting the split using the batch_size:\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "\tdata = train_data if split == 'train' else val_data\n",
        "\tix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\tprint(ix)\n",
        "\tx = torch.stack([data[i:i+block_size] for i in ix])\n",
        "\ty = torch.stack([data[i+1:block_size + 1] for i in ix])\n",
        "\treturn x,y\n",
        "\n",
        "x,y = get_batch('train')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU0suiXWYEX_",
        "outputId": "e8db09b7-d8a1-47ba-c55d-cce0f2980c2a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([158077, 127223,  22710, 145684])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are done setting up the batched data, we can now build the  we are using Neural Model\n"
      ],
      "metadata": {
        "id": "LUnxQrQBZhyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):#note the class inheritance\n",
        "  def __init__(self, vocabulary_size):\n",
        "    super().__init__() #required if we are to create new argumnets when using class inheritance\n",
        "    self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)#creating our embedded layer but it's empty\n",
        "\n",
        "  #note that it's important that we set up our forward propagation ourselves\n",
        "  def forward(self, index, targets = None):\n",
        "    logits = self.token_embedding_table(index) #this is us simply populating the embedded layer with info from the training data\n",
        "\n",
        "    if targets is  None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "        B,T,C = logits.shape #getting the shape elements of the populated embedded layer\n",
        "        logits = logits.view(B*T,C) #now reshapeing the shape of the populated embedded layer\n",
        "        targets = targets.view(B*T) #reshaping the shape of the target, noting that it wasn't feed into an embeded layer at all\n",
        "        loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self,index, max_new_tokens):\n",
        "    '''apparently, the generate function is designed to generate new tokens (words or characters), given an initial context.\n",
        "    It samples from the probability distribution of possible next tokens at each step to create a sequence of text.\n",
        "     This is useful in language modeling tasks, such as predicting or generating the next token based on previous tokens.\n",
        "\t '''\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self.forward(index)\n",
        "      '''This line below extracts the logits corresponding to the last token in the sequence (the most recent one).\n",
        "       In the bigram model, the model predicts the next token based on the current token, so the last token is the most relevant one.'''\n",
        "      logits = logits[:,-1,:] #pop quiz: how was the 2nd element of the embedding layer specified even though its not possible to get an embedding layer with 2 dimensions (as it have been to by the forward function )\n",
        "\n",
        "      probs = F.softmax(logits, dim = -1) # to convert the logits into probabilities over the vocabulary, so you can sample the next token.\n",
        "\n",
        "      index_next = torch.multinomial(probs, num_samples=1) #samples a token based on the probability distribution. This allows the model to predict the next token with a degree of randomness.\n",
        "\n",
        "      index = torch.cat((index, index_next), dim = 1) #After generating a new token, it is concatenated with the existing sequence (index) to form an updated context for the next iteration.\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "hQXaumi5Zl8_"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now running the generate method & checking to see the nature of the result:\n",
        "model = BigramLanguageModel(vocabulary_size)\n",
        "\n",
        "context = torch.zeros((1,1), dtype = torch.long)\n",
        "\n",
        "generated_chars = decode(model.generate(context, max_new_tokens = 500)[0].tolist())\n",
        "\n",
        "print(generated_chars)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZZF9FJrdiYR",
        "outputId": "1d22b1ce-5ba9-4815-b556-0d946bee2817"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "”1PveFmb]Okb15i:WuL5iN m][”lVKPQuwf“yXPQFuKDP“qeb.ZHpDiEV!1UUV)300.JjRDPCvfN﻿;l\n",
            ";FfgxPWdE:yxhQFaq?3sgy&L“,[aNV﻿mS.eX”aDw!5s[z](rio#I&-mt”1UqY—y0(?vIXb,?W3jNpz9Unb]&:Q?QYnfh﻿Qg#q&A5gE[I’Y TQ﻿BTqBBBXHRE2Kif-mIIgTXCYKFr-g\n",
            "‘n“rvNy[&p2KG!u,GMt.eYK?yFj”5gT”hDxgT2dHFu\n",
            "G”lWY﻿\n",
            "BIQ#E2”&jKWg#)WdZBYK?g[xxlyHa:!in]K—IvDvL3w,?Z1HIq(HXHXq9BNS.tke ]Jc!“O\n",
            "&qCRU:b;FiE[vXl(cths”S:yrlD(f-Zz,kvf9nNeTCj,?‘yw-mMErDDiWodFVee﻿‘sg)I’MprGg\n",
            "N?\n",
            "-Z—eaONpSuCCe]gy9O(YwuN’[VwEW”bMoqe;E3“Mu9tW#JaTi\n",
            "Ju,“&qn]k0Vh0?nR-m(WaqhWke(z##\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now creating the pytorch optimizer:\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "jyAgXRqzgh7n"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocabulary_size)"
      ],
      "metadata": {
        "id": "OV6Lxc86tDcg"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now creating the training loop:\n",
        "for iter in range(max_iters):\n",
        "  # sample a batch of data:\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss:\n",
        "  logits, loss = model.forward(xb, yb)  # forward pass\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)  # This clears (zeroes out) the gradients of all the parameters before a new training step.\n",
        "  # note: Setting set_to_none=True is more efficient than resetting gradients to zero because it sets the gradients to None, saving memory and computation. This is beneficial when certain gradients aren't required in that step; when it is not RNN\n",
        "\n",
        "  loss.backward()  # backward pass\n",
        "  optimizer.step()  # performs a step of the gradient descent\n",
        "\n",
        "print(loss.item())  # to see the final loss after training\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qJLd4XsXcCgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now running the generate method & checking to see the nature of the result:\n",
        "model = BigramLanguageModel(vocabulary_size)\n",
        "\n",
        "context = torch.zeros((1,1), dtype = torch.long)\n",
        "\n",
        "generated_chars = decode(model.generate(context, max_new_tokens = 500)[0].tolist())\n",
        "\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "qVRwfeogmAno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}